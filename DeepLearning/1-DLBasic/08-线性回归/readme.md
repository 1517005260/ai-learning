# 要点总结

## 线性回归

- 最小二乘法[推导](https://zhuanlan.zhihu.com/p/420140010)

![最小二乘法](https://i0.hdslb.com/bfs/note/dd613e93bb91fd628df08ec2e01e65fd00627f68.jpg)

- 线性回归损失函数是凸函数，所以有最优解。**本课程之后所有的 loss 函数都是没有最优解的。**因为有最优解的函数求解都比较简单，没有必要上神经网络。
- 显示解 => 显式解

## 简单优化方法——梯度下降

- 梯度：上升最快的方向。**负梯度**：下降最快的方向
- 学习率 η：代表每步能走多远，需要人为选定
- 看 ppt 的图，η \* d(loss)/d(wt-1) 就是一段距离，wt-1 加上这段距离到达了 wt
- 梯度下降是不需要知道解的形式（显式解）的，只要根据程序一步步逼近最优解、降低 loss 即可
- 重要参数：学习率、批大小
