# 感知机

- 最早的 AI 模型
- 二分类问题
- 有监督
- 灵感来源于人类神经元

感知机是一种线性分类模型。也就是说针对的是线性可分的数据。最常见的例子就是二维平面内的一堆数据可以通过一条直线分割开。

感知机不能处理异或问题。

- 激活函数：将输出转为 T/F 的函数，如 ppt σ(x)所示

**训练感知机的参数更新**：

- loss = max(0, − y⟨w, x⟩) （单个点，总 loss 求和即可）
- 这里的 max 对应 ppt 里的 if 语句：如果 y 大于 0，那么分类正确，取 max 后 loss 为 0，无需更新参数。如果 y 小于 0，那么分类错误，取 max 后 loss 大于 0，需要进行参数更新
- 对应到 ppt 猫狗分类的例子：一开始给定一对猫狗，分类完成（以线分隔）。之后来了一只狗，但是落在了原来猫的区域，故分类错误，计算机重新调整分隔线，使之适应给出的猫狗分类。
  - 分类正确：所有的数据样本都落在正确的区域内 ==> **收敛定理**，指的是经过有限次迭代以后，可以得到一个正确划分正负样本的线（不唯一）
  - 收敛定理参数：r-数据范围、大小；ρ-数据质量、即两点分隔得开不开
  - 分隔线： wx + b = 0
  - ppt 中的限制：1）w ^ 2 + b ^ 2 <= 1:用于正规化模型参数，防止过拟合、保证数据稳定性 2）规定 ρ：数据点在分类时至少要保持这个距离离开决策边界，保证训练强健性

# 多层感知机 MLP，Multilayer perceptron

- 现在的主流深度学习模型，MLP 是各种其他 AI 模型之母

**多重感知机的优势：学习 XOR**

如 ppt，多步学习，x 轴是特征 1，y 轴是特征 2，经过黄色和蓝色层的学习后得到灰色层的结果。

即，从原来的简单函数变到了复合函数（多了隐藏层）。输入和输出都是定死的，因为都是由数据和目的输出决定的，我们唯一能改变的就是中间的隐藏层。

**激活函数不能是线性的**（y=nx + b）：详见 ppt 证明，否则最后多隐藏层的结果会和简单线性模型一样（层数塌陷）。但是最后一层（output）一般是不用激活函数的。

- 一般而言，我们还会更倾向于更光滑的函数，因为更容易求导
- 特别的，我们会使用 ReLU 函数（折线不是线性函数，分段线性引入了多层），因为求导相比于指数求导快得多

**注意**：名称 trick：线性回归 + 隐藏层 = 单类分类； softmax 回归 + 隐藏层 = 多类分类

- 深度学习是在做压缩，所以一般而言，靠近输出的隐藏层要比靠近输入的隐藏层小（逐步压缩）。**金字塔**，如果先压缩再扩张，可能会损失较多信息。
- 一般而言，我们会选择多层隐藏层而不是单层巨大的隐藏层，尽管二者是等价的，毕竟是“深度学习”而不是“宽度学习”，而且前者比较容易过拟合。而对于深度的神经网络，我们每次可以让一层网络专门学一个点，这样效果可能会好一点。比如，学习分类猫狗图片的时候，可能第一层学怎么分辨耳朵，第二层学怎么分辨头...最后的效果可能会比较好。
