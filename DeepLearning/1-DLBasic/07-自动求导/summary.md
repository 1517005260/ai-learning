# 矩阵求导

## 奇淫技巧总结

- 重点是搞清楚最后矩阵/向量的形状
- 如果求导的对象不涉及“值”（内积、二范数等），最后结果的矩阵不需要转置；反之需要转置

## 自动求导

- 神经网络一般都有几百层，可以看作是一个一百层的复合函数，所以手动链式法则求导比较困难。
- 显式构造：像是你先画好一个路线图，确定每个节点和连接的路径，然后让数据沿着这张图走。
- 隐式构造：像是你走一步画一步，每到一个新地点才决定下一步要走的方向，逐步绘制出整个路线图。

在 PyTorch 中，隐式构造的方式更加自然和普遍，而显式构造更像是一种低层次的操作，用于特定需求的场景。

## 链式法则

- 正向传递：从 x 出发（正向：输入 x --> 输出 y）**预测**
- 反向传递：从 y 出发（反向：输出 y --> 输入 x）**学习**

正常高数学的求导方法就是反向传递

反向传递内存复杂度 O(n)， 因为要存下所有中间结果，这也是深度学习要耗 GPU 的罪魁祸首

正向传播不会对每一层计算梯度，所以深度学习里用的比较少（梯度用于求最快最小 loss）

# 正向传播与反向传播总结

## 1. 正向传播与反向传播简介

| **方面**         | **正向传播 (Forward Propagation)**             | **反向传播 (Backpropagation)**                             |
| ---------------- | ---------------------------------------------- | ---------------------------------------------------------- |
| **定义**         | 将输入数据逐层传递到输出，计算出模型的预测值。 | 从输出开始，通过链式法则逐层计算梯度，用于更新模型参数。   |
| **目的**         | 计算模型的输出（预测值）。                     | 计算损失函数关于每个参数的梯度，并通过这些梯度来优化模型。 |
| **过程**         | 输入 -> 层 1 -> 层 2 -> ... -> 输出 (y)        | 损失函数 L -> 输出(y) -> 层 n -> ... -> 层 1 -> 参数更新   |
| **使用场景**     | 模型预测、推断                                 | 模型训练、参数优化                                         |
| **空间复杂度**   | \(O(1)\)                                       | \(O(n)\)                                                   |
| **时间复杂度**   | \(O(n)\)                                       | \(O(n)\)                                                   |
| **是否计算梯度** | 否                                             | 是                                                         |

## 2. 计量经济学与深度学习的对比

| **领域**         | **计量经济学**                                     | **深度学习**                                       |
| ---------------- | -------------------------------------------------- | -------------------------------------------------- |
| **正向传播应用** | 主要用于预测，如线性回归中的计算。                 | 用于模型的预测阶段，计算输出结果。                 |
| **反向传播应用** | 通常不涉及反向传播；参数通过闭式解或数值方法估计。 | 核心训练方法，通过链式法则计算梯度，更新网络参数。 |
| **模型复杂度**   | 通常较低，结构固定，参数较少。                     | 高度非线性，模型复杂，参数众多。                   |
| **学习过程**     | 参数学习依赖于特定的优化方法，如最小二乘法。       | 学习过程依赖于梯度下降法和反向传播的结合。         |
| **主要优化方法** | 闭式解（如最小二乘）、简单的数值优化。             | 通过反向传播和梯度下降，进行迭代优化。             |
| **计算方式**     | 通常一次性计算出结果（解析解）。                   | 通过迭代逐步更新参数，依赖于反向传播。             |

## 3. 正向传播与反向传播的区别

| **方面**         | **正向传播 (Forward Propagation)** | **反向传播 (Backpropagation)**     |
| ---------------- | ---------------------------------- | ---------------------------------- |
| **数据流向**     | 从输入流向输出                     | 从输出流向输入，反向计算           |
| **核心任务**     | 预测                               | 计算梯度，更新参数                 |
| **链式法则应用** | 不涉及                             | 广泛应用，通过链式法则逐层计算梯度 |
| **计算对象**     | 输入数据和当前模型参数             | 当前模型参数和损失函数             |
