{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 y 是标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0) # x 是列向量\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导： y = 2 [X.T] [X]，显然y是标量，x是向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在反向传播之前，我们需要有地方来存储梯度\n",
    "x.requires_grad_(True) # 开启自动微分\n",
    "x.grad # 默认None，以后可以通过这个属性访问x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad_fn=<MulBackward0>`这个是y的属性，表示y是在前向传播（x->y）中通过乘法操作生成的，这个属性会在反向传播时帮助 PyTorch 确定如何计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  4.,  8., 12.]), tensor(28., grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用反向传播函数计算y关于x的各个分量的梯度，结果存入x.grad中\n",
    "y.backward()\n",
    "x.grad, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标答应该就是4，y = 2 X.T X 关于X求导应该是 4X\n",
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，pytorch 会将梯度累计，我们如要计算另外的梯度，需要手动清零\n",
    "x.grad.zero_()\n",
    "y = x.sum() # y = x1 + x2 +...\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看看梯度是怎么累计的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重复一下：\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch会默认累计梯度，这是他们设计上的理念，是为了处理大批量的梯度时能够分步计算而累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 y 是向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论上，向量y关于向量x的导数应该是一个矩阵，但是在深度学习中，我们很少会这么做。\n",
    "\n",
    "大部分情况我们都是对一个标量来求导（loss一般是标量，向量loss比较麻烦），所以y是向量时，一般先求和再求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 1., 2., 3.], requires_grad=True))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().backward()  # y = x1 ^2 + x2 ^2 + ...\n",
    "# 等价于y.backward(torch.ones(len(x))), 即y和一个全1向量作点积后再求导\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 分离计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即： y = f(x), z = g(y,x) 我们想求z对x的梯度\n",
    "\n",
    "思路：把y看成常数，而不是x的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.]),\n",
       " tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = y.detach()  # u = [0,1,4,9]，但是u不在x,y的计算图里，是个常数\n",
    "z = u * x\n",
    "u, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，z对x求导，把u当成了常数。而y还是x的函数，所以y还可以对x求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Python控制流的梯度计算\n",
    "\n",
    "隐式构造的优势\n",
    "\n",
    "PyTorch 的自动微分功能如何处理带有条件分支和循环的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动构造一个控制流：\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:  # b的每个元素的平方和再开根号\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1571, requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True) # 一个随机标量\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 隐式构造：当定义并执行 f(a) 这个函数时，PyTorch 并不预先知道整个计算图的形状和大小。相反，它随着代码的执行，**逐步**构建计算图。\n",
    "    - 例如，在 while b.norm() < 1000: 这个循环中，PyTorch会根据 b 的当前值决定是否继续执行循环，并且每次迭代时都会动态地添加相应的操作到计算图中。\n",
    "\n",
    "所以，我们有如下结论：在 PyTorch 中，计算图是动态构建的，这意味着计算图可以在代码执行时，根据控制流（如 `if` 语句、`while` 循环）动态生成。PyTorch 能够追踪这些操作并自动计算梯度。这种特性使得 PyTorch 在处理复杂的控制流时非常灵活和强大。\n",
    "\n",
    "流程总结：a开启了自动微分，然后进入f函数，此时，对a做的所有操作都是前向传播，而且被记录到计算图中了，计算图也在此时被构建，所有的结果存入d。然后d.backward()会基于前向的路径反向走，对a求导。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
